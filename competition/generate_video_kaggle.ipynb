{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: hdf5plugin in /home/vscode/.local/lib/python3.11/site-packages (5.0.0)\n",
      "Requirement already satisfied: opencv-python-headless in /home/vscode/.local/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: moviepy in /home/vscode/.local/lib/python3.11/site-packages (1.0.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/AE4353/lib/python3.11/site-packages (4.66.5)\n",
      "Requirement already satisfied: h5py>=3.0.0 in /opt/conda/envs/AE4353/lib/python3.11/site-packages (from hdf5plugin) (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/envs/AE4353/lib/python3.11/site-packages (from opencv-python-headless) (1.26.4)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/vscode/.local/lib/python3.11/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/envs/AE4353/lib/python3.11/site-packages (from moviepy) (2.32.3)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/envs/AE4353/lib/python3.11/site-packages (from moviepy) (2.33.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/vscode/.local/lib/python3.11/site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/envs/AE4353/lib/python3.11/site-packages (from imageio<3.0,>=2.5->moviepy) (10.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/AE4353/lib/python3.11/site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (72.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/AE4353/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/AE4353/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/AE4353/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/AE4353/lib/python3.11/site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install hdf5plugin opencv-python-headless moviepy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the `moviepy`, you need to restart the kernel for the video writer to work properly. You can restart by going to \"More settings\" (three dots button at the top right corner) and click \"Restart and Clear Cell Outputs\". After restarted, you do not need to install the packages again and can continue from this cell onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import hdf5plugin\n",
    "import matplotlib.pyplot as plt\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load external model\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model_trainedRMSprop3_epoch.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5_to_video(h5, output_dir, fps):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # get images and targets\n",
    "    h5f = h5py.File(h5, \"r\")\n",
    "    images = h5f[\"images\"]\n",
    "    targets = [h5f[f\"targets/{i:05d}\"][()] for i in range(len(images))]\n",
    "\n",
    "    # draw targets on images\n",
    "    frames = []\n",
    "    for image, target in tqdm(zip(images, targets), total=len(images)):\n",
    "\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        frame = image.copy()\n",
    "\n",
    "        for gate in target2:\n",
    "            xy = gate.reshape(-1, 3)[..., :2] * image.shape[1::-1]\n",
    "            visibility = gate.reshape(-1, 3)[..., 2]\n",
    "            if np.all(visibility > 0):\n",
    "                cv2.polylines(\n",
    "                    frame,\n",
    "                    [xy.astype(int)],\n",
    "                    isClosed=True,\n",
    "                    color=(0, 255, 0),\n",
    "                    thickness=2,\n",
    "                )\n",
    "            else:\n",
    "                # Draw lines between visible corners\n",
    "                for i in range(len(xy) - 1):\n",
    "                    if visibility[i] > 0 and visibility[i + 1] > 0:\n",
    "                        cv2.line(\n",
    "                            frame,\n",
    "                            tuple(xy[i].astype(int)),\n",
    "                            tuple(xy[i + 1].astype(int)),\n",
    "                            color=(0, 255, 0),\n",
    "                            thickness=2,\n",
    "                        )\n",
    "                # Draw line from the last corner to the first to form a loop if both are visible\n",
    "                if visibility[-1] > 0 and visibility[0] > 0:\n",
    "                    cv2.line(\n",
    "                        frame,\n",
    "                        tuple(xy[-1].astype(int)),\n",
    "                        tuple(xy[0].astype(int)),\n",
    "                        color=(0, 255, 0),\n",
    "                        thickness=2,\n",
    "                    )\n",
    "        frames.append(frame)\n",
    "\n",
    "    h5f.close()\n",
    "\n",
    "    clip = ImageSequenceClip(frames, fps=fps)\n",
    "    clip.write_videofile(str(output_dir / f\"{h5.stem}.mp4\"), codec=\"libx264\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/802 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 480, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x1200 and 614400x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m h5_to_video(\n\u001b[1;32m      2\u001b[0m     Path(\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspaces/AE4353-Y24/competition/data/Autonomous/autonomous_flight-01a-ellipse.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m     ),\n\u001b[1;32m      5\u001b[0m     Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspaces/AE4353-Y24/competition/data/Autonomous/\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n",
      "Cell \u001b[0;32mIn[27], line 15\u001b[0m, in \u001b[0;36mh5_to_video\u001b[0;34m(h5, output_dir, fps)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(image2\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     target2 \u001b[38;5;241m=\u001b[39m model(image2)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(target2)\n\u001b[1;32m     17\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/AE4353/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/AE4353/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/AE4353-Y24/competition/NeuralNetwork.py:57\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten -> [batch, 512 * (H/16) * (W/16)]\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Regression layers\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))  \u001b[38;5;66;03m# Fully connected layer -> [batch, 1024]\u001b[39;00m\n\u001b[1;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)  \u001b[38;5;66;03m# Final layer -> [batch, num_targets * 12]\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Reshape to [batch, num_targets, 12]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/AE4353/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/AE4353/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/AE4353/lib/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x1200 and 614400x1024)"
     ]
    }
   ],
   "source": [
    "h5_to_video(\n",
    "    Path(\n",
    "        \"/workspaces/AE4353-Y24/competition/data/Autonomous/autonomous_flight-01a-ellipse.h5\"\n",
    "    ),\n",
    "    Path(\"/workspaces/AE4353-Y24/competition/data/Autonomous\"),\n",
    "    30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9635081,
     "sourceId": 85326,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
